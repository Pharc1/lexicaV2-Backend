from flask import Blueprint, jsonify, request, Response
import logging
from openai import OpenAI
from chromadb.utils import embedding_functions
import os
import chromadb
from storage import save_discussion, get_discussions_history, delete_discussion,append_message_to_discussion
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cr√©ation d'un blueprint Flask pour le module principal
main = Blueprint('main', __name__)

# Initialisation du client OpenAI
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# D√©finition du chemin pour la base de donn√©es Chroma
# Configuration du client Chroma
chroma_host = os.getenv('CHROMA_DB_HOST', 'localhost')  
chroma_port = os.getenv('CHROMA_DB_PORT', 8000) 
chroma_client = chromadb.HttpClient(host=chroma_host, port=chroma_port)
embeddings_model = embedding_functions.OpenAIEmbeddingFunction(model_name="text-embedding-3-small", api_key=os.getenv('OPENAI_API_KEY'))


@main.route('/health')
def health_check():
    """Endpoint de v√©rification de la sant√© de l'API."""
    return jsonify({"status": "healthy", "message": "API is running"}), 200

@main.route('/ask', methods=['POST'])
def ask():
    """Traite les demandes de questions de l'utilisateur.

    Returns:
        Response: R√©ponse g√©n√©r√©e par l'API OpenAI ou un message d'erreur.
    """
    try:
        # R√©cup√©rer la question depuis le body JSON
        data = request.get_json()
        if not data or 'question' not in data:
            return jsonify({"error": "Aucune question fournie."}), 400

        question = data.get('question')
        if not question.strip():
            return jsonify({"error": "La question ne peut pas √™tre vide."}), 400

        discussion_path = data.get('filename')
        logger.info(f"Question re√ßue: {question}")

        base_instructions = """
        Tu es Lexica, un assistant bienveillant qui vouvoie toujours et r√©pond avec joie.
        Ton cr√©ateur est Pharci Un ing√©nieur en Inteligence artificielle.
        Tu fournis uniquement des r√©ponses bas√©es sur tes connaissances. 
        Si une question d√©passe tes connaissances, tu l'indiques gentiment. 
        Tu n'as pas toujours besoin du contexte trouv√©, si tu as des informations mais qu'on te parle naturellement tu parles naturellement aussi.
        Tu r√©ponds toujours avec du markdown tres stylis√© et organis√© avec toutes sortes de balises afin de rendre le texte agreables.
        Ton but principale est d'aider les utilisateur grace √† ta source de connaissance.
        """

        # Recherche dans ChromaDB
        collection = chroma_client.get_or_create_collection(
            name="Documents", 
            embedding_function=embeddings_model
        )

        results = collection.query(
            query_texts=[question],
            n_results=5
        )

        documents = results.get('documents', [[]])[0]
        metadatas = results.get('metadatas', [[]])[0]
        distances = results.get('distances', [[]])[0]

        # üí• Seuil de similarit√©
        seuil = 1

        # üîç Filtrage des r√©sultats pertinents
        filtered_docs = []
        filtered_metas = []

        for doc, meta, dist in zip(documents, metadatas, distances):
            print(dist)
            if dist < seuil:
                filtered_docs.append(doc)
                filtered_metas.append(meta)

        # üí¨ Pr√©paration du contexte uniquement si au moins un document pertinent
        # Pr√©parer les messages
        messages = []

        # üîπ Ajouter le contexte au prompt system
        if filtered_docs:
                context = "\n\n----\n\n".join(filtered_docs)
                filenames = list({meta.get('filename') for meta in filtered_metas if meta.get('filename')})
                logger.info("Contexte trouv√©: %s", context[:200] + "...")
                context_with_instructions = base_instructions + "\n\nConnaissances :\n" + context
        else:
                context = ""
                filenames = []
                logger.info("Aucun contexte assez pertinent trouv√©, Lexica r√©pondra sans.")
                context_with_instructions = base_instructions

        # üîπ Ajout du message syst
        messages.append({"role": "system", "content": context_with_instructions})

        # üîπ Gestion de l‚Äôhistorique
        if "messages" in data and isinstance(data["messages"], list):
                previous_messages = data["messages"]
        
            # ‚ö†Ô∏è On garde uniquement les r√¥les user/assistant
                for msg in previous_messages:
                    if msg["role"] in ("user", "assistant"):
                            messages.append(msg)

        # üîπ Ajouter la nouvelle question
        messages.append({"role": "user", "content": question})

        # üîπ Sauvegarder la question
        append_message_to_discussion(discussion_path, {"type": "user", "content": question})

        # üîπ Pour stocker la r√©ponse de l‚Äôassistant
        full_response = ""

        # üîπ Streaming
        def generate():
            nonlocal full_response
            try:
                stream = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    stream=True,
                )

                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield content

            except Exception as e:
                logger.error("Erreur mod√®le : %s", str(e))
                error_message = "Une erreur est survenue lors du traitement."
                full_response = error_message
                save_discussion(question, error_message, context)
                yield error_message

        filename_header = "||".join(filenames)

        # üîπ On sauvegarde la r√©ponse une fois g√©n√©r√©e
        def stream_with_save():
                for chunk in generate():
                    yield chunk
                append_message_to_discussion(discussion_path, {"type": "assistant", "content": full_response})
                save_discussion(question, full_response, context)

        return Response(
                stream_with_save(),
                content_type="text/plain",
                headers={
                    "Cache-Control": "no-cache, no-store, must-revalidate",
                    "Access-Control-Allow-Origin": "https://lexica.pharci.fr",  # PAS "*"
                    "Access-Control-Allow-Headers": "X-Used-Filenames, Content-Type",
                    "Access-Control-Expose-Headers": "X-Used-Filenames",  # üí• pour que le frontend puisse lire ce header
                    "Access-Control-Allow-Credentials": "true",
                    "X-Used-Filenames": filename_header
                }
        )
 
    except Exception as e:
        logger.error("Erreur lors de la recherche de similarit√© : %s", str(e))
        return jsonify({"error": "Erreur lors de la recherche de similarit√©."}), 500

@main.route('/history/discussions', methods=['GET'])
def get_discussions():
    """R√©cup√®re l'historique des discussions."""
    try:
        limit = request.args.get('limit', 10, type=int)
        discussions = get_discussions_history(limit)
        return jsonify({
            "discussions": discussions,
            "count": len(discussions)
        }), 200
    except Exception as e:
        logger.error("Erreur lors de la r√©cup√©ration de l'historique : %s", str(e))
        return jsonify({"error": "Erreur lors de la r√©cup√©ration de l'historique"}), 500

@main.route('/history/discussions/<discussion_id>', methods=['DELETE'])
def delete_discussion_endpoint(discussion_id):
    """Supprime une discussion."""
    try:
        success = delete_discussion(discussion_id)
        if success:
            return jsonify({"message": "Discussion supprim√©e avec succ√®s"}), 200
        else:
            return jsonify({"error": "Discussion non trouv√©e"}), 404
    except Exception as e:
        logger.error("Erreur lors de la suppression de la discussion : %s", str(e))
        return jsonify({"error": "Erreur lors de la suppression de la discussion"}), 500
